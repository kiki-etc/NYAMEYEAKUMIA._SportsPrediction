{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor, AdaBoostRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, mean_absolute_error, r2_score\n",
    "import pickle as pkl\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Problem: FIFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zr/tt4xly5d0jl0yn_5q0tn8hch0000gn/T/ipykernel_8359/3264449872.py:1: DtypeWarning: Columns (25,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  training_data = pandas.read_csv(\"players_22.csv\")\n",
      "/var/folders/zr/tt4xly5d0jl0yn_5q0tn8hch0000gn/T/ipykernel_8359/3264449872.py:2: DtypeWarning: Columns (108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  testing_data  = pandas.read_csv(\"male_players (legacy).csv\")\n"
     ]
    }
   ],
   "source": [
    "training_data = pandas.read_csv(\"players_22.csv\")\n",
    "testing_data  = pandas.read_csv(\"male_players (legacy).csv\")\n",
    "\n",
    "training_data = training_data[['player_url','short_name','long_name','player_positions','potential','value_eur','wage_eur','age','dob','height_cm','weight_kg','club_team_id','club_name','league_name','league_level','club_position','club_jersey_number','club_loaned_from','nationality_id','nationality_name','nation_team_id','nation_position','nation_jersey_number','preferred_foot','weak_foot','skill_moves','international_reputation','work_rate','body_type','real_face','release_clause_eur','player_tags','player_traits','pace','shooting','passing','dribbling','defending','physic','attacking_crossing','attacking_finishing','attacking_heading_accuracy','attacking_short_passing','attacking_volleys','skill_dribbling','skill_curve','skill_fk_accuracy','skill_long_passing','skill_ball_control','movement_acceleration','movement_sprint_speed','movement_agility','movement_reactions','movement_balance','power_shot_power','power_jumping','power_stamina','power_strength','power_long_shots','mentality_aggression','mentality_interceptions','mentality_positioning','mentality_vision','mentality_penalties','mentality_composure','defending_marking_awareness','defending_standing_tackle','defending_sliding_tackle','goalkeeping_diving','goalkeeping_handling','goalkeeping_kicking','goalkeeping_positioning','goalkeeping_reflexes','goalkeeping_speed','ls','st','rs','lw','lf','cf','rf','rw','lam','cam','ram','lm','lcm','cm','rcm','rm','lwb','ldm','cdm','rdm','rwb','lb','lcb','cb','rcb','rb','gk','player_face_url','overall']]\n",
    "testing_data  = testing_data[ ['player_url','short_name','long_name','player_positions','potential','value_eur','wage_eur','age','dob','height_cm','weight_kg','club_team_id','club_name','league_name','league_level','club_position','club_jersey_number','club_loaned_from','nationality_id','nationality_name','nation_team_id','nation_position','nation_jersey_number','preferred_foot','weak_foot','skill_moves','international_reputation','work_rate','body_type','real_face','release_clause_eur','player_tags','player_traits','pace','shooting','passing','dribbling','defending','physic','attacking_crossing','attacking_finishing','attacking_heading_accuracy','attacking_short_passing','attacking_volleys','skill_dribbling','skill_curve','skill_fk_accuracy','skill_long_passing','skill_ball_control','movement_acceleration','movement_sprint_speed','movement_agility','movement_reactions','movement_balance','power_shot_power','power_jumping','power_stamina','power_strength','power_long_shots','mentality_aggression','mentality_interceptions','mentality_positioning','mentality_vision','mentality_penalties','mentality_composure','defending_marking_awareness','defending_standing_tackle','defending_sliding_tackle','goalkeeping_diving','goalkeeping_handling','goalkeeping_kicking','goalkeeping_positioning','goalkeeping_reflexes','goalkeeping_speed','ls','st','rs','lw','lf','cf','rf','rw','lam','cam','ram','lm','lcm','cm','rcm','rm','lwb','ldm','cdm','rdm','rwb','lb','lcb','cb','rcb','rb','gk','player_face_url','overall']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "manually dropping columns with irrelevant data because they, intuitively, bare no correlation with a player's rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.drop(columns=['player_url','short_name','long_name','dob','club_team_id','club_name','league_name','league_level','club_position','club_jersey_number','club_loaned_from','nationality_id','nationality_name','nation_team_id','nation_position','nation_jersey_number','real_face','release_clause_eur','player_tags','player_traits','player_face_url'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop columns that have over 30% null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = (training_data.isnull().sum()/len(training_data))\n",
    "drop = percentage[percentage > 0.3].index\n",
    "training_data.drop(columns=drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zr/tt4xly5d0jl0yn_5q0tn8hch0000gn/T/ipykernel_8359/442175344.py:16: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered = filtered.applymap(convert_to_number)\n"
     ]
    }
   ],
   "source": [
    "filtered = training_data.filter(items=['ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram',\n",
    "                                    'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb',\n",
    "                                    'lb', 'lcb', 'cb', 'rcb', 'rb', 'gk'])\n",
    "\n",
    "def convert_to_number(entry):\n",
    "    entry = str(entry)  # convert entry to string\n",
    "    if '+' in entry:\n",
    "        parts = entry.split('+')\n",
    "        return int(parts[0]) + int(parts[1])\n",
    "    elif '-' in entry:\n",
    "        parts = entry.split('-')\n",
    "        return int(parts[0]) - int(parts[1])\n",
    "    else:\n",
    "        return int(entry)\n",
    "\n",
    "filtered = filtered.applymap(convert_to_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.update(filtered)\n",
    "int_columns = ['ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram',\n",
    "                'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb',\n",
    "                'lb', 'lcb', 'cb', 'rcb', 'rb', 'gk']\n",
    "\n",
    "training_data[int_columns] = training_data[int_columns].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric      = training_data.select_dtypes(include=numpy.number)\n",
    "non_numeric  = training_data.select_dtypes(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imputing the missing numerical values with the **mean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_with_NaN = numeric.columns[numeric.isnull().any().tolist()]\n",
    "\n",
    "for column in numeric_with_NaN:\n",
    "   numeric[column].fillna(numeric[column].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replacing NaN `object` values with the **mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric_with_NaN = non_numeric.columns[non_numeric.isnull().any().tolist()]\n",
    "\n",
    "for column in non_numeric_with_NaN:\n",
    "  non_numeric[column].fillna(non_numeric[column].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**encoding** for non-numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for column in non_numeric:\n",
    "  non_numeric[column] = label_encoder.fit_transform(non_numeric[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concatenating to form the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_positions</th>\n",
       "      <th>preferred_foot</th>\n",
       "      <th>work_rate</th>\n",
       "      <th>body_type</th>\n",
       "      <th>potential</th>\n",
       "      <th>value_eur</th>\n",
       "      <th>wage_eur</th>\n",
       "      <th>age</th>\n",
       "      <th>height_cm</th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>...</th>\n",
       "      <th>cdm</th>\n",
       "      <th>rdm</th>\n",
       "      <th>rwb</th>\n",
       "      <th>lb</th>\n",
       "      <th>lcb</th>\n",
       "      <th>cb</th>\n",
       "      <th>rcb</th>\n",
       "      <th>rb</th>\n",
       "      <th>gk</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>604</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>93</td>\n",
       "      <td>78000000.0</td>\n",
       "      <td>320000.0</td>\n",
       "      <td>34</td>\n",
       "      <td>170</td>\n",
       "      <td>72</td>\n",
       "      <td>...</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>69</td>\n",
       "      <td>64</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>64</td>\n",
       "      <td>22</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>635</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>92</td>\n",
       "      <td>119500000.0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>32</td>\n",
       "      <td>185</td>\n",
       "      <td>81</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>67</td>\n",
       "      <td>64</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>64</td>\n",
       "      <td>22</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>658</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>91</td>\n",
       "      <td>45000000.0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>187</td>\n",
       "      <td>83</td>\n",
       "      <td>...</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>66</td>\n",
       "      <td>63</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>63</td>\n",
       "      <td>23</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>372</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>91</td>\n",
       "      <td>129000000.0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>29</td>\n",
       "      <td>175</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>70</td>\n",
       "      <td>65</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>65</td>\n",
       "      <td>23</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>168</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>91</td>\n",
       "      <td>125500000.0</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>30</td>\n",
       "      <td>181</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>82</td>\n",
       "      <td>78</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>78</td>\n",
       "      <td>24</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   player_positions  preferred_foot  work_rate  body_type  potential  \\\n",
       "0               604               0          7          9         93   \n",
       "1               635               1          2          9         92   \n",
       "2               658               1          1          9         91   \n",
       "3               372               1          2          9         91   \n",
       "4               168               1          0          9         91   \n",
       "\n",
       "     value_eur  wage_eur  age  height_cm  weight_kg  ...  cdm  rdm  rwb  lb  \\\n",
       "0   78000000.0  320000.0   34        170         72  ...   67   67   69  64   \n",
       "1  119500000.0  270000.0   32        185         81  ...   69   69   67  64   \n",
       "2   45000000.0  270000.0   36        187         83  ...   62   62   66  63   \n",
       "3  129000000.0  270000.0   29        175         68  ...   66   66   70  65   \n",
       "4  125500000.0  350000.0   30        181         70  ...   83   83   82  78   \n",
       "\n",
       "   lcb  cb  rcb  rb  gk  overall  \n",
       "0   53  53   53  64  22       93  \n",
       "1   63  63   63  64  22       92  \n",
       "2   56  56   56  63  23       91  \n",
       "3   53  53   53  65  23       91  \n",
       "4   72  72   72  78  24       91  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = pandas.concat([non_numeric, numeric], axis=1)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function for cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def clean_player_data(data):\n",
    "    \"\"\"\n",
    "    The `clean_player_data` function takes a DataFrame as a parameter and returns a DataFrame with the clean data (no missing values and non-numeric data is encoded). It only works if the column data is similar, so the following columns need to be present: ['player_url','short_name','long_name','player_positions','potential','value_eur','wage_eur','age','dob','height_cm','weight_kg','club_team_id','club_name','league_name','league_level','club_position','club_jersey_number','club_loaned_from','nationality_id','nationality_name','nation_team_id','nation_position','nation_jersey_number','preferred_foot','weak_foot','skill_moves','international_reputation','work_rate','body_type','real_face','release_clause_eur','player_tags','player_traits','pace','shooting','passing','dribbling','defending','physic','attacking_crossing','attacking_finishing','attacking_heading_accuracy','attacking_short_passing','attacking_volleys','skill_dribbling','skill_curve','skill_fk_accuracy','skill_long_passing','skill_ball_control','movement_acceleration','movement_sprint_speed','movement_agility','movement_reactions','movement_balance','power_shot_power','power_jumping','power_stamina','power_strength','power_long_shots','mentality_aggression','mentality_interceptions','mentality_positioning','mentality_vision','mentality_penalties','mentality_composure','defending_marking_awareness','defending_standing_tackle','defending_sliding_tackle','goalkeeping_diving','goalkeeping_handling','goalkeeping_kicking','goalkeeping_positioning','goalkeeping_reflexes','goalkeeping_speed','ls','st','rs','lw','lf','cf','rf','rw','lam','cam','ram','lm','lcm','cm','rcm','rm','lwb','ldm','cdm','rdm','rwb','lb','lcb','cb','rcb','rb','gk','player_face_url','overall']\n",
    "    data: the DataFrame with similar column headings to the `players_22.csv` dataset\n",
    "    \"\"\"\n",
    "    data.drop(columns=['player_url', 'short_name', 'long_name', 'dob', 'club_team_id', 'club_name', 'league_name', \n",
    "                       'league_level', 'club_position', 'club_jersey_number', 'club_loaned_from', 'nationality_id', \n",
    "                       'nationality_name', 'nation_team_id', 'nation_position', 'nation_jersey_number', 'real_face', \n",
    "                       'release_clause_eur', 'player_tags', 'player_traits', 'player_face_url'], inplace=True)\n",
    "\n",
    "    # drop columns with more than 30% missing values\n",
    "    percentage = (data.isnull().sum() / len(data))\n",
    "    drop = percentage[percentage > 0.3].index\n",
    "    data.drop(columns=drop, inplace=True)\n",
    "\n",
    "    int_columns = ['ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram',\n",
    "                   'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb',\n",
    "                   'lb', 'lcb', 'cb', 'rcb', 'rb', 'gk']\n",
    "    filtered_columns = data.filter(items=int_columns)\n",
    "\n",
    "    def convert_to_int(entry):\n",
    "        entry = str(entry)  # Convert entry to string\n",
    "        if '+' in entry:\n",
    "            parts = entry.split('+')\n",
    "            return int(parts[0]) + int(parts[1])\n",
    "        elif '-' in entry:\n",
    "            parts = entry.split('-')\n",
    "            return int(parts[0]) - int(parts[1])\n",
    "        else:\n",
    "            return int(entry)\n",
    "\n",
    "    filtered_columns = filtered_columns.applymap(convert_to_int)\n",
    "\n",
    "    # update data with converted columns\n",
    "    data.update(filtered_columns)\n",
    "    data[int_columns] = data[int_columns].astype(int)\n",
    "\n",
    "    numeric_data = data.select_dtypes(include=np.number)\n",
    "    non_numeric_data = data.select_dtypes(include=['object'])\n",
    "\n",
    "    # fill missing values in numeric columns with mean\n",
    "    numeric_NaN = numeric_data.columns[numeric_data.isnull().any()].tolist()\n",
    "    for column in numeric_NaN:\n",
    "        numeric_data[column].fillna(numeric_data[column].mean(), inplace=True)\n",
    "\n",
    "    # fill missing values in non-numeric columns with mode\n",
    "    non_numeric_NaN = non_numeric_data.columns[non_numeric_data.isnull().any()].tolist()\n",
    "    for column in non_numeric_NaN:\n",
    "        non_numeric_data[column].fillna(non_numeric_data[column].mode()[0], inplace=True)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    for column in non_numeric_data:\n",
    "        non_numeric_data[column] = label_encoder.fit_transform(non_numeric_data[column])\n",
    "\n",
    "    return pd.concat([non_numeric_data, numeric_data], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaning the testing data using the `clean_player_data` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zr/tt4xly5d0jl0yn_5q0tn8hch0000gn/T/ipykernel_8359/1261613747.py:36: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_columns = filtered_columns.applymap(convert_to_int)\n"
     ]
    }
   ],
   "source": [
    "testing_data = clean_player_data(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Using the RandomForest classifier to decide which features are the most important, as opposed to the correlation matrix, which may be too simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ytrain = training_data.drop('overall',axis=1), training_data['overall']\n",
    "Xtest,  Ytest  = testing_data.drop('overall',axis=1),  testing_data['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rforest = RandomForestRegressor(n_estimators=110, random_state=45, max_depth=15, criterion='absolute_error')\n",
    "rforest.fit(Xtrain, Ytrain)\n",
    "\n",
    "feature_importances = rforest.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>value_eur</td>\n",
       "      <td>0.593809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>age</td>\n",
       "      <td>0.140090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>potential</td>\n",
       "      <td>0.116538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>movement_reactions</td>\n",
       "      <td>0.094952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>gk</td>\n",
       "      <td>0.006534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wage_eur</td>\n",
       "      <td>0.003026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>rb</td>\n",
       "      <td>0.001988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>lb</td>\n",
       "      <td>0.001945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>lcb</td>\n",
       "      <td>0.001551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>cb</td>\n",
       "      <td>0.001525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Feature  Importance\n",
       "5            value_eur    0.593809\n",
       "7                  age    0.140090\n",
       "4            potential    0.116538\n",
       "32  movement_reactions    0.094952\n",
       "79                  gk    0.006534\n",
       "6             wage_eur    0.003026\n",
       "78                  rb    0.001988\n",
       "74                  lb    0.001945\n",
       "75                 lcb    0.001551\n",
       "76                  cb    0.001525"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_df = pd.DataFrame({'Feature': Xtrain.columns, 'Importance': feature_importances}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "feature_importance_df.iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparing with correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall                 1.000000\n",
       "movement_reactions      0.871823\n",
       "mentality_composure     0.708867\n",
       "passing                 0.663519\n",
       "potential               0.644275\n",
       "                          ...   \n",
       "goalkeeping_diving     -0.010990\n",
       "goalkeeping_handling   -0.011080\n",
       "goalkeeping_kicking    -0.012986\n",
       "preferred_foot         -0.048961\n",
       "work_rate              -0.227014\n",
       "Name: overall, Length: 81, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = pd.DataFrame(training_data.corr())\n",
    "corr['overall'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = ['movement_reactions', 'mentality_composure', 'power_shot_power', 'cm', 'mentality_vision',\n",
    "                     'value_eur', 'age', 'potential', 'gk', 'wage_eur', 'overall']\n",
    "\n",
    "Xtrain = training_data[selected]\n",
    "Xtest  = testing_data[selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "Xtrain_scaled = scaler.fit_transform(Xtrain)\n",
    "Xtest_scaled  = scaler.fit_transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the ensemble models\n",
    "Picking three ensemble models, then tuning the hyper parameters to get the best predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.002037215700197425\n",
      "MSE:  0.004801986163161281\n",
      "RMSE: 0.06929636471822516\n",
      "R2 =  0.9999029823402199\n"
     ]
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "gbr.fit(Xtrain, Ytrain)\n",
    "gbr_initial = gbr.predict(Xtest)\n",
    "\n",
    "print(f\"\"\"MAE:  {mean_absolute_error(gbr_initial, Ytest)}\n",
    "MSE:  {mean_squared_error(gbr_initial, Ytest)}\n",
    "RMSE: {numpy.sqrt(mean_squared_error(gbr_initial, Ytest))}\n",
    "R2 =  {r2_score(gbr_initial, Ytest)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_parameters = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.1, 0.05]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.00176237136889556\n",
      "MSE:  0.004825140312538382\n",
      "RMSE: 0.06946322993165796\n",
      "R2 =  0.9999025254427534\n"
     ]
    }
   ],
   "source": [
    "grid_search_gb = GridSearchCV(estimator=gbr, param_grid=gbr_parameters, cv=kf, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_gb.fit(Xtrain, Ytrain)\n",
    "\n",
    "gbr_best_parameters = grid_search_gb.best_params_\n",
    "\n",
    "# training with best parameters\n",
    "best_gbr = GradientBoostingRegressor(**gbr_best_parameters, random_state=42)\n",
    "best_gbr.fit(Xtrain, Ytrain)\n",
    "\n",
    "# prediction and evaluation\n",
    "y_pred_gbr = best_gbr.predict(Xtest)\n",
    "print(f\"\"\"MAE:  {mean_absolute_error(y_pred_gbr, Ytest)}\n",
    "MSE:  {mean_squared_error(y_pred_gbr, Ytest)}\n",
    "RMSE: {numpy.sqrt(mean_squared_error(y_pred_gbr, Ytest))}\n",
    "R2 =  {r2_score(y_pred_gbr, Ytest)}\"\"\")\n",
    "\n",
    "gbr_evaluator = numpy.sqrt(mean_squared_error(y_pred_gbr, Ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adaptive Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.5828337239652456\n",
      "MSE:  0.6133103858804542\n",
      "RMSE: 0.7831413575341646\n",
      "R2 =  0.9872677719086972\n"
     ]
    }
   ],
   "source": [
    "ada = AdaBoostRegressor()\n",
    "\n",
    "ada.fit(Xtrain, Ytrain)\n",
    "ada_initial = ada.predict(Xtest)\n",
    "\n",
    "print(f\"\"\"MAE:  {mean_absolute_error(ada_initial, Ytest)}\n",
    "MSE:  {mean_squared_error(ada_initial, Ytest)}\n",
    "RMSE: {numpy.sqrt(mean_squared_error(ada_initial, Ytest))}\n",
    "R2 =  {r2_score(ada_initial, Ytest)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_parameters = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [1.0, 0.5, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.008717051641867304\n",
      "MSE:  0.004970000013279849\n",
      "RMSE: 0.07049822702224397\n",
      "R2 =  0.9998996027444854\n"
     ]
    }
   ],
   "source": [
    "grid_search_ada = GridSearchCV(estimator=ada, param_grid=ada_parameters, cv=kf, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_ada.fit(Xtrain, Ytrain)\n",
    "\n",
    "ada_best_parameters = grid_search_ada.best_params_\n",
    "\n",
    "# training with best parameters\n",
    "best_ada = GradientBoostingRegressor(**ada_best_parameters, random_state=42)\n",
    "best_ada.fit(Xtrain, Ytrain)\n",
    "\n",
    "# prediction and evaluation\n",
    "y_pred_ada = best_ada.predict(Xtest)\n",
    "print(f\"\"\"MAE:  {mean_absolute_error(y_pred_ada, Ytest)}\n",
    "MSE:  {mean_squared_error(y_pred_ada, Ytest)}\n",
    "RMSE: {numpy.sqrt(mean_squared_error(y_pred_ada, Ytest))}\n",
    "R2 =  {r2_score(y_pred_ada, Ytest)}\"\"\")\n",
    "\n",
    "ada_evaluator = numpy.sqrt(mean_squared_error(y_pred_ada, Ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Histogram Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.006967419409874487\n",
      "MSE:  0.011065834020021549\n",
      "RMSE: 0.10519426799983708\n",
      "R2 =  0.9997763353086572\n"
     ]
    }
   ],
   "source": [
    "hgb = HistGradientBoostingRegressor()\n",
    "\n",
    "hgb.fit(Xtrain, Ytrain)\n",
    "hgb_initial = hgb.predict(Xtest)\n",
    "\n",
    "print(f\"\"\"MAE:  {mean_absolute_error(hgb_initial, Ytest)}\n",
    "MSE:  {mean_squared_error(hgb_initial, Ytest)}\n",
    "RMSE: {numpy.sqrt(mean_squared_error(hgb_initial, Ytest))}\n",
    "R2 =  {r2_score(hgb_initial, Ytest)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgb_parameters = {\n",
    "    'learning_rate': [1.0, 0.5, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.002089769783337002\n",
      "MSE:  0.004829647227291388\n",
      "RMSE: 0.06949566337039591\n",
      "R2 =  0.999902435993156\n"
     ]
    }
   ],
   "source": [
    "grid_search_hgb = GridSearchCV(estimator=hgb, param_grid=hgb_parameters, cv=kf, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_hgb.fit(Xtrain, Ytrain)\n",
    "\n",
    "hgb_best_parameters = grid_search_hgb.best_params_\n",
    "\n",
    "# training with best parameters\n",
    "best_hgb = GradientBoostingRegressor(**hgb_best_parameters, random_state=42)\n",
    "best_hgb.fit(Xtrain, Ytrain)\n",
    "\n",
    "# prediction and evaluation\n",
    "y_pred_hgb = best_hgb.predict(Xtest)\n",
    "print(f\"\"\"MAE:  {mean_absolute_error(y_pred_hgb, Ytest)}\n",
    "MSE:  {mean_squared_error(y_pred_hgb, Ytest)}\n",
    "RMSE: {numpy.sqrt(mean_squared_error(y_pred_hgb, Ytest))}\n",
    "R2 =  {r2_score(y_pred_hgb, Ytest)}\"\"\")\n",
    "\n",
    "hgb_evaluator = numpy.sqrt(mean_squared_error(y_pred_hgb, Ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model:           GradientBoostingRegressor(n_estimators=200, random_state=42)\n",
      "Best Hyperparameters: {'learning_rate': 0.1, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "evaluators = [gbr_evaluator, ada_evaluator, hgb_evaluator]\n",
    "minimum_rmse = min(evaluators)\n",
    "\n",
    "if minimum_rmse == gbr_evaluator:\n",
    "    best_model  = best_gbr\n",
    "    best_params = gbr_best_parameters\n",
    "elif minimum_rmse == ada_evaluator:\n",
    "    best_model  = best_ada\n",
    "    best_params = ada_best_parameters\n",
    "else:\n",
    "    best_model  = best_hgb\n",
    "    best_params = hgb_best_parameters\n",
    "\n",
    "y_pred_test = best_model.predict(Xtest)\n",
    "mse_test = mean_squared_error(Ytest, y_pred_test)\n",
    "\n",
    "print(f\"\"\"Best Model:           {best_model}\n",
    "Best Hyperparameters: {best_params}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model (`.pkl` file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl.dump(best_model, open(best_model.__class__.__name__ + '.pkl', 'wb'))\n",
    "joblib.dump(scaler, 'scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
